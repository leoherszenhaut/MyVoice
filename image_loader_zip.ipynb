{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import io\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import cycle\n",
    "from time import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import Input, Model\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = r'C:/Users/leohe/Documents/ITC/HIVE/Challenge/data/asl_alphabet_train.zip'\n",
    "\n",
    "LABEL = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9,\n",
    "         'K':10, 'L':11, 'M':12, 'N':13, 'O':14, 'P':15, 'Q':16, 'R':17, 'S':18, 'T':19,\n",
    "         'U':20, 'V':21, 'W':22, 'X':23, 'Y':24, 'Z':25, \n",
    "         'del':26, 'nothing':27, 'space':28}\n",
    "\n",
    "N = 87000 # total number of images in the training set\n",
    "IMG_SIZE = 200 # original image size, to be resized\n",
    "IMG_CHANNEL = 3 # rgb color scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idos_filter(img, *options):\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loading the images into a np.array from inside the zip file\n",
    "- loading and translating the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                        | 0/87030 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image space130.jpg   , from label 28, was read, filtered, formatted and stacked.     1/87000\n",
      "Image space13.jpg    , from label 28, was read, filtered, formatted and stacked.     2/87000\n",
      "Image space1298.jpg  , from label 28, was read, filtered, formatted and stacked.     3/87000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "stacked  = np.zeros((N, IMG_SIZE, IMG_SIZE, IMG_CHANNEL), dtype='uint8')\n",
    "label = np.zeros((N, 1), dtype='uint8')\n",
    "with ZipFile(FILENAME) as archive:\n",
    "    for entry in tqdm(archive.infolist()):\n",
    "        name = str(entry).split(\"'\")[1].replace('/', ' ').split()\n",
    "        if len(name) == 3:\n",
    "            with archive.open(entry) as file:\n",
    "                img = idos_filter(np.array(Image.open(file)))\n",
    "                stacked[count] = img\n",
    "                label[count] = LABEL[name[1]]\n",
    "                count += 1\n",
    "                print(\"Image {:15s}, from label {:2d}, was read, \"\n",
    "                      \"filtered, formatted and stacked. {:5d}/87000\"\n",
    "                      .format(name[-1], LABEL[name[1]], count))\n",
    "            if count >= 3:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = shuffle(np.arange(len(label)))\n",
    "# stacked, label = shuffle(stacked, label)\n",
    "idx_train, idx_test = train_test_split(idx, test_size=0.01)\n",
    "print(len(idx_train), len(idx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=stacked[idx_train].shape[1:])\n",
    "f = layers.Conv2D(64, (3, 3), kernel_initializer='glorot_normal')(inputs)\n",
    "f = layers.BatchNormalization()(f)\n",
    "f = layers.Conv2D(32, (3, 3), kernel_initializer='glorot_normal', activation='relu')(f)\n",
    "f = layers.BatchNormalization()(f)\n",
    "f = layers.MaxPool2D()(f)\n",
    "f = layers.Conv2D(16, (3, 3), kernel_initializer='glorot_normal', activation='relu')(f)\n",
    "f = layers.BatchNormalization()(f)\n",
    "f = layers.Conv2D( 8, (3, 3), kernel_initializer='glorot_normal', activation='relu')(f)\n",
    "f = layers.BatchNormalization()(f)\n",
    "f = layers.MaxPool2D()(f)\n",
    "f = layers.Flatten()(f)\n",
    "f = layers.Dense(400, activation='relu', kernel_initializer='glorot_normal')(f)\n",
    "f = layers.Dense(200, activation='relu', kernel_initializer='glorot_normal')(f)\n",
    "f = layers.Dense(100, activation='relu', kernel_initializer='glorot_normal')(f)\n",
    "f = layers.Dense(100, activation='relu', kernel_initializer='glorot_normal')(f)\n",
    "f = layers.Dense( 50, activation='relu', kernel_initializer='glorot_normal')(f)\n",
    "outputs = layers.Dense(len(np.unique(label[idx_train])), activation='softmax')(f)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x=stacked[idx_train] / np.max(stacked[idx_train]), \n",
    "                    y=to_categorical(label[idx_train], len(np.unique(label[idx_train]))), \n",
    "                    batch_size=128, epochs=20,) \n",
    "#                     validation_split=0.2)\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
